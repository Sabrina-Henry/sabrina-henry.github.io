{"title":"What does it mean to explain a prediction?","markdown":{"yaml":{"title":"","page-layout":"article","format":{"html":{"toc":false,"number-sections":false}}},"headingText":"What does it mean to explain a prediction?","containsRefs":false,"markdown":"\n\n\n```{=html}\n<div class=\"article-page\">\n  <a class=\"article-home\" href=\"/\">← Home</a>\n  <div class=\"article-hero\">\n\n    <h1>Explaining Deep Neural Networks by Following Confidence</h1>\n    <h2>A self-guided path approach to model attribution</h2>\n\n    <div class=\"article-meta\">\n      <div class=\"meta-grid\">\n\n        <div>\n          <span class=\"meta-label\">Author</span>\n          <p>Sabrina Henry</p> \n          \n        </div>\n\n        <div>\n          <span class=\"meta-label\">Affiliation</span>\n          <p>Heriot-Watt University</p>\n        </div>\n\n        <div>\n          <span class=\"meta-label\">Article Written</span>\n          <p>February 2025</p>\n        </div>\n\n      </div>\n    </div>\n\n  </div>\n```\n\nModern neural networks can have very high confidence — but confidence alone does not tell us *why* a prediction was made.\n\nFeature attribution methods aim to answer this *why* by highlighting which parts of an input most influenced a model’s decision. In practice, however, the way we choose to explain a prediction can actually shape the explanation itself.\n\n\n\nGiven an input and a trained model, an attribution method assigns importance scores to individual input features — pixels in an image, time points on a graph, or tokens in text — indicating how strongly each contributed to the final prediction.\n\nAmong the most widely used approaches are *path-based* methods, which accumulate model gradients as the input is gradually transformed, telling us how sensitive each input feature is to the output prediction. \n\n```{=html}\n<div class=\"interactive-figure\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/ladybug.png\"\n      alt=\"Input image\"\n      class=\"figure-image base-image\"\n    >\n\n    <img\n      src=\"Interactive_figs/SIGMA_ladybug.png\"\n      alt=\"Integrated Gradients attribution\"\n      class=\"figure-image attribution-image\"\n    >\n  </div>\n\n  <button class=\"toggle-btn\" onclick=\"toggleAttribution(this)\">\n    Click to Show Attribution\n  </button>\n\n    <figcaption>\n    The model predicted the class 'Ladybug' for this input image, click to highlight which pixels were most inportant in this prediction.\n  </figcaption>\n\n</div>\n```\n\n\n\n\n\n## The baseline assumption\n\nIntegrated Gradients, a widely used path-based method, explains a prediction by integrating gradients along a straight path from a *baseline* input to the image being explained.\n\nThe baseline is intended to represent the absence of information — often a black or blurred image. However, different baselines can lead to noticeably different explanations, even when the model and input remain unchanged.\n\n\n```{=html}\n<figure class=\"interactive-figure\" data-figure=\"baseline-1\">\n\n  <div class=\"figure-frame-overlay\">\n    <img\n      src=\"Interactive_figs/ladybug.png\"\n      alt=\"Input image\"\n      class=\"figure-image base-image\"\n    >\n\n    <!-- One overlay that we swap -->\n    <img\n      src=\"\"\n      alt=\"Attribution overlay\"\n      class=\"figure-image attribution-image\"\n      data-overlay\n    >\n  </div>\n\n  <div class=\"toggle-row\" role=\"group\" aria-label=\"Choose baseline\">\n    <button class=\"toggle-btn baseline-btn\"\n            aria-pressed=\"true\"\n            data-src=\"Interactive_figs/IG_black_baseline.png\"\n            onclick=\"setBaseline(this)\">\n      Black\n    </button>\n\n    <button class=\"toggle-btn baseline-btn\"\n            aria-pressed=\"false\"\n            data-src=\"Interactive_figs/IG_noise_baseline.png\"\n            onclick=\"setBaseline(this)\">\n      Noise\n    </button>\n\n    <button class=\"toggle-btn baseline-btn\"\n            aria-pressed=\"false\"\n            data-src=\"Interactive_figs/IG_blur_baseline.png\"\n            onclick=\"setBaseline(this)\">\n      Blurred\n    </button>\n  </div>\n\n  <figcaption>\n    Integrated Gradients attribution for the same input using different baselines, click to see the effect of baseline choice.\n  </figcaption>\n\n</figure>\n```\n\n\n## Saturation along attribution paths\n\nA less visible issue arises from how models behave along these paths. As the input moves away from the baseline, the model’s confidence often increases rapidly before entering a saturated region where further changes have little effect on the output.\n\nGradients accumulated in these flat regions can dominate the final attribution, despite contributing little to the actual decision. This can introduce noise and obscure the features that truly matter.\n\n\n```{=html}\n<figure class=\"interactive-figure figure-large\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/saturation-01.png\"\n      alt=\"SIGMA attribution for the ladybug image\"\n      class=\"figure-image\"\n    >\n  </div>\n\n</figure>\n```\n\n\n## Thinking in terms of confidence landscapes\n\nRather than viewing attribution as interpolation between two images, it can be helpful to think of the model’s output as a *confidence landscape* over input space.\n\nSome directions produce steep changes in confidence, while others lie along broad plateaus. An informative explanation should focus on the regions where the model’s belief actually changes — not where it is already certain.\n\n\n```{=html}\n<figure class=\"interactive-figure figure-large\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/conflandscape.png\"\n      alt=\"SIGMA attribution for the ladybug image\"\n      class=\"figure-image\"\n    >\n  </div>\n\n</figure>\n```\n\n\n## A self-guided path\n\nThe Self-Guided Integrated Gradient Method for Attribution (SIGMA) removes the need for a baseline entirely.\n\nInstead of following a predefined path, SIGMA constructs its own trajectory by iteratively perturbing the input in directions that reduce the model’s confidence in the predicted class. Gradients are accumulated along this path and weighted by the corresponding drop in confidence, ensuring that each step contributes proportionally to the explanation.\n\n```{=html}\n\n<figure class=\"interactive-figure figure-video\">\n  <video\n    class=\"article-video\"\n    autoplay\n    muted\n    loop\n    playsinline\n    controls\n    preload=\"auto\"\n  >\n    <source src=\"Interactive_figs/sigma_side_by_side.mp4\" type=\"video/mp4\">\n    Sorry — your browser can’t play this video.\n  </video>\n\n  <figcaption>\n    SIGMA trajectory through a prediction landscape with corresponding perturbed input and attribution evolution.\n  </figcaption>\n</figure>\n```\n```{=html}\n<figure class=\"interactive-figure figure-method\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/SIGMA.png\"\n      alt=\"SIGMA attribution method\"\n      class=\"figure-image\"\n    >\n  </div>\n\n\n</figure>\n```\n\n\n## What changes in practice?\n\nBecause SIGMA follows the model’s confidence downhill, it avoids early saturation and continues to collect informative gradients throughout the path.\n\nThe resulting attribution maps tend to be more spatially coherent and less influenced by regions that have little effect on the prediction. Importantly, this behaviour emerges directly from the model’s own response, rather than from external design choices.\n\n```{=html}\n<figure class=\"interactive-figure figure-large\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/further_results_qualitative.png\"\n      alt=\"SIGMA attribution for the ladybug image\"\n      class=\"figure-image\"\n    >\n  </div>\n\n</figure>\n```\n\n## Faithfulness to model behaviour\n\nAn explanation is only meaningful if it reflects how the model actually behaves.\n\nBy accumulating gradients in proportion to confidence change, SIGMA aligns attribution strength with the model’s sensitivity. Revealing input features in order of attribution importance reconstructs the model’s confidence more efficiently than random or saturated-gradient explanations.\n\n## Beyond explanation\n\nFollowing confidence collapse naturally produces perturbed inputs that remain recognisable to humans but receive near-zero confidence from the model.\n\nThese low-confidence variants expose decision boundaries and can be reused as training augmentations. Incorporating them during retraining improves robustness to noise and adversarial perturbations, linking interpretability and model reliability. SIGMA provides robustness to both gaussian noise and targeted adveserial attacks such as those using the Fast Gradient Sign Method (FGSM), suggesting that SIGMA provides a balanced regularisation between noise-based and adversarial strategies. Click on the images below to highlight how augmenting with that perturbation type provides robustness to various noise conditions and targeted attacks. \n\n```{=html}\n<div class=\"perturb-widget\">\n\n  <!-- a) image buttons -->\n  <div class=\"perturb-buttons\" role=\"tablist\" aria-label=\"Choose perturbation\">\n    <button class=\"perturb-btn is-active\" data-mode=\"clean\" aria-selected=\"true\">\n      <img src=\"Interactive_figs/clean_button.png\" alt=\"Clean example\">\n    </button>\n\n    <button class=\"perturb-btn\" data-mode=\"gaussian\" aria-selected=\"false\">\n      <img src=\"Interactive_figs/gaussian_button.png\" alt=\"Gaussian example\">\n    </button>\n\n    <button class=\"perturb-btn\" data-mode=\"fgsm\" aria-selected=\"false\">\n      <img src=\"Interactive_figs/fgsm_button.png\" alt=\"FGSM example\">\n    </button>\n\n    <button class=\"perturb-btn\" data-mode=\"sigma\" aria-selected=\"false\">\n      <img src=\"Interactive_figs/sigma_button.png\" alt=\"SIGMA example\">\n    </button>\n\n  </div>\n\n  \n  <figure class=\"chart-figure\">\n    <img id=\"chartImg\"\n         src=\"Interactive_figs/web_bar_original.png\"\n         alt=\"Bar chart for Clean perturbation\">\n    <figcaption id=\"chartCap\">Original network performance (Clean)</figcaption>\n  </figure>\n</div>\n\n<script>\n(function () {\n  \n  const CHARTS = {\n    clean:    { src: \"Interactive_figs/web_bar_original.png\",    cap: \"Original network performance (Clean)\" },\n    gaussian: { src: \"Interactive_figs/web_bar_gaussian.png\", cap: \"Network performance under Gaussian noise augmentations\" },\n    fgsm:     { src: \"Interactive_figs/web_bar_FGSM.png\",     cap: \"Network performance under FGSM augmentations\" },\n    sigma:    { src: \"Interactive_figs/web_bar_SIGMA.png\",    cap: \"Network performance under SIGMA augmentations\" }\n  };\n\n  const chartImg = document.getElementById(\"chartImg\");\n  const chartCap = document.getElementById(\"chartCap\");\n\n  function setActive(btn) {\n    document.querySelectorAll(\".perturb-btn\").forEach(b => {\n      const active = (b === btn);\n      b.classList.toggle(\"is-active\", active);\n      b.setAttribute(\"aria-selected\", active ? \"true\" : \"false\");\n    });\n  }\n\n  function swapChart(mode) {\n    const d = CHARTS[mode];\n\n    // nice: fade transition\n    chartImg.classList.add(\"is-fading\");\n    const next = new Image();\n    next.onload = () => {\n      chartImg.src = d.src;\n      chartImg.alt = `Bar chart for ${mode}`;\n      chartCap.textContent = d.cap;\n\n      requestAnimationFrame(() => {\n        chartImg.classList.remove(\"is-fading\");\n      });\n    };\n    next.src = d.src; // preload then swap\n  }\n\n  document.querySelectorAll(\".perturb-btn\").forEach(btn => {\n    btn.addEventListener(\"click\", () => {\n      const mode = btn.dataset.mode;\n      setActive(btn);\n      swapChart(mode);\n    });\n  });\n})();\n</script>\n```\n\n## Conclusion\n\nSIGMA offers a shift in perspective for attribution based interpretability, from explaining predictions relative to arbitrary references, to explaining them by following the model’s own confidence.\n\nBy treating explanations as paths shaped by the model itself, we gain both clearer attributions and a deeper understanding of how confidence emerges and collapses. This can also be used in model augmentation to ensure a network is more robust to noise or targeted adveserial attacks. \n\n\n---\n\n**Paper:** [Self-Guided Integrated Gradient Method for Attribution (preprint)](https://www.techrxiv.org/doi/full/10.36227/techrxiv.175037106.61341481)\n\n**Code:** [GitHub repository](https://github.com/HWQuantum/SIGMA)\n\n**Contact:** [sjh9@hw.ac.uk](mailto:sjh9@hw.ac.uk)\n\n</div>","srcMarkdownNoYaml":"\n\n\n```{=html}\n<div class=\"article-page\">\n  <a class=\"article-home\" href=\"/\">← Home</a>\n  <div class=\"article-hero\">\n\n    <h1>Explaining Deep Neural Networks by Following Confidence</h1>\n    <h2>A self-guided path approach to model attribution</h2>\n\n    <div class=\"article-meta\">\n      <div class=\"meta-grid\">\n\n        <div>\n          <span class=\"meta-label\">Author</span>\n          <p>Sabrina Henry</p> \n          \n        </div>\n\n        <div>\n          <span class=\"meta-label\">Affiliation</span>\n          <p>Heriot-Watt University</p>\n        </div>\n\n        <div>\n          <span class=\"meta-label\">Article Written</span>\n          <p>February 2025</p>\n        </div>\n\n      </div>\n    </div>\n\n  </div>\n```\n\nModern neural networks can have very high confidence — but confidence alone does not tell us *why* a prediction was made.\n\nFeature attribution methods aim to answer this *why* by highlighting which parts of an input most influenced a model’s decision. In practice, however, the way we choose to explain a prediction can actually shape the explanation itself.\n\n\n## What does it mean to explain a prediction?\n\nGiven an input and a trained model, an attribution method assigns importance scores to individual input features — pixels in an image, time points on a graph, or tokens in text — indicating how strongly each contributed to the final prediction.\n\nAmong the most widely used approaches are *path-based* methods, which accumulate model gradients as the input is gradually transformed, telling us how sensitive each input feature is to the output prediction. \n\n```{=html}\n<div class=\"interactive-figure\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/ladybug.png\"\n      alt=\"Input image\"\n      class=\"figure-image base-image\"\n    >\n\n    <img\n      src=\"Interactive_figs/SIGMA_ladybug.png\"\n      alt=\"Integrated Gradients attribution\"\n      class=\"figure-image attribution-image\"\n    >\n  </div>\n\n  <button class=\"toggle-btn\" onclick=\"toggleAttribution(this)\">\n    Click to Show Attribution\n  </button>\n\n    <figcaption>\n    The model predicted the class 'Ladybug' for this input image, click to highlight which pixels were most inportant in this prediction.\n  </figcaption>\n\n</div>\n```\n\n\n\n\n\n## The baseline assumption\n\nIntegrated Gradients, a widely used path-based method, explains a prediction by integrating gradients along a straight path from a *baseline* input to the image being explained.\n\nThe baseline is intended to represent the absence of information — often a black or blurred image. However, different baselines can lead to noticeably different explanations, even when the model and input remain unchanged.\n\n\n```{=html}\n<figure class=\"interactive-figure\" data-figure=\"baseline-1\">\n\n  <div class=\"figure-frame-overlay\">\n    <img\n      src=\"Interactive_figs/ladybug.png\"\n      alt=\"Input image\"\n      class=\"figure-image base-image\"\n    >\n\n    <!-- One overlay that we swap -->\n    <img\n      src=\"\"\n      alt=\"Attribution overlay\"\n      class=\"figure-image attribution-image\"\n      data-overlay\n    >\n  </div>\n\n  <div class=\"toggle-row\" role=\"group\" aria-label=\"Choose baseline\">\n    <button class=\"toggle-btn baseline-btn\"\n            aria-pressed=\"true\"\n            data-src=\"Interactive_figs/IG_black_baseline.png\"\n            onclick=\"setBaseline(this)\">\n      Black\n    </button>\n\n    <button class=\"toggle-btn baseline-btn\"\n            aria-pressed=\"false\"\n            data-src=\"Interactive_figs/IG_noise_baseline.png\"\n            onclick=\"setBaseline(this)\">\n      Noise\n    </button>\n\n    <button class=\"toggle-btn baseline-btn\"\n            aria-pressed=\"false\"\n            data-src=\"Interactive_figs/IG_blur_baseline.png\"\n            onclick=\"setBaseline(this)\">\n      Blurred\n    </button>\n  </div>\n\n  <figcaption>\n    Integrated Gradients attribution for the same input using different baselines, click to see the effect of baseline choice.\n  </figcaption>\n\n</figure>\n```\n\n\n## Saturation along attribution paths\n\nA less visible issue arises from how models behave along these paths. As the input moves away from the baseline, the model’s confidence often increases rapidly before entering a saturated region where further changes have little effect on the output.\n\nGradients accumulated in these flat regions can dominate the final attribution, despite contributing little to the actual decision. This can introduce noise and obscure the features that truly matter.\n\n\n```{=html}\n<figure class=\"interactive-figure figure-large\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/saturation-01.png\"\n      alt=\"SIGMA attribution for the ladybug image\"\n      class=\"figure-image\"\n    >\n  </div>\n\n</figure>\n```\n\n\n## Thinking in terms of confidence landscapes\n\nRather than viewing attribution as interpolation between two images, it can be helpful to think of the model’s output as a *confidence landscape* over input space.\n\nSome directions produce steep changes in confidence, while others lie along broad plateaus. An informative explanation should focus on the regions where the model’s belief actually changes — not where it is already certain.\n\n\n```{=html}\n<figure class=\"interactive-figure figure-large\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/conflandscape.png\"\n      alt=\"SIGMA attribution for the ladybug image\"\n      class=\"figure-image\"\n    >\n  </div>\n\n</figure>\n```\n\n\n## A self-guided path\n\nThe Self-Guided Integrated Gradient Method for Attribution (SIGMA) removes the need for a baseline entirely.\n\nInstead of following a predefined path, SIGMA constructs its own trajectory by iteratively perturbing the input in directions that reduce the model’s confidence in the predicted class. Gradients are accumulated along this path and weighted by the corresponding drop in confidence, ensuring that each step contributes proportionally to the explanation.\n\n```{=html}\n\n<figure class=\"interactive-figure figure-video\">\n  <video\n    class=\"article-video\"\n    autoplay\n    muted\n    loop\n    playsinline\n    controls\n    preload=\"auto\"\n  >\n    <source src=\"Interactive_figs/sigma_side_by_side.mp4\" type=\"video/mp4\">\n    Sorry — your browser can’t play this video.\n  </video>\n\n  <figcaption>\n    SIGMA trajectory through a prediction landscape with corresponding perturbed input and attribution evolution.\n  </figcaption>\n</figure>\n```\n```{=html}\n<figure class=\"interactive-figure figure-method\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/SIGMA.png\"\n      alt=\"SIGMA attribution method\"\n      class=\"figure-image\"\n    >\n  </div>\n\n\n</figure>\n```\n\n\n## What changes in practice?\n\nBecause SIGMA follows the model’s confidence downhill, it avoids early saturation and continues to collect informative gradients throughout the path.\n\nThe resulting attribution maps tend to be more spatially coherent and less influenced by regions that have little effect on the prediction. Importantly, this behaviour emerges directly from the model’s own response, rather than from external design choices.\n\n```{=html}\n<figure class=\"interactive-figure figure-large\">\n\n  <div class=\"figure-frame\">\n    <img\n      src=\"Interactive_figs/further_results_qualitative.png\"\n      alt=\"SIGMA attribution for the ladybug image\"\n      class=\"figure-image\"\n    >\n  </div>\n\n</figure>\n```\n\n## Faithfulness to model behaviour\n\nAn explanation is only meaningful if it reflects how the model actually behaves.\n\nBy accumulating gradients in proportion to confidence change, SIGMA aligns attribution strength with the model’s sensitivity. Revealing input features in order of attribution importance reconstructs the model’s confidence more efficiently than random or saturated-gradient explanations.\n\n## Beyond explanation\n\nFollowing confidence collapse naturally produces perturbed inputs that remain recognisable to humans but receive near-zero confidence from the model.\n\nThese low-confidence variants expose decision boundaries and can be reused as training augmentations. Incorporating them during retraining improves robustness to noise and adversarial perturbations, linking interpretability and model reliability. SIGMA provides robustness to both gaussian noise and targeted adveserial attacks such as those using the Fast Gradient Sign Method (FGSM), suggesting that SIGMA provides a balanced regularisation between noise-based and adversarial strategies. Click on the images below to highlight how augmenting with that perturbation type provides robustness to various noise conditions and targeted attacks. \n\n```{=html}\n<div class=\"perturb-widget\">\n\n  <!-- a) image buttons -->\n  <div class=\"perturb-buttons\" role=\"tablist\" aria-label=\"Choose perturbation\">\n    <button class=\"perturb-btn is-active\" data-mode=\"clean\" aria-selected=\"true\">\n      <img src=\"Interactive_figs/clean_button.png\" alt=\"Clean example\">\n    </button>\n\n    <button class=\"perturb-btn\" data-mode=\"gaussian\" aria-selected=\"false\">\n      <img src=\"Interactive_figs/gaussian_button.png\" alt=\"Gaussian example\">\n    </button>\n\n    <button class=\"perturb-btn\" data-mode=\"fgsm\" aria-selected=\"false\">\n      <img src=\"Interactive_figs/fgsm_button.png\" alt=\"FGSM example\">\n    </button>\n\n    <button class=\"perturb-btn\" data-mode=\"sigma\" aria-selected=\"false\">\n      <img src=\"Interactive_figs/sigma_button.png\" alt=\"SIGMA example\">\n    </button>\n\n  </div>\n\n  \n  <figure class=\"chart-figure\">\n    <img id=\"chartImg\"\n         src=\"Interactive_figs/web_bar_original.png\"\n         alt=\"Bar chart for Clean perturbation\">\n    <figcaption id=\"chartCap\">Original network performance (Clean)</figcaption>\n  </figure>\n</div>\n\n<script>\n(function () {\n  \n  const CHARTS = {\n    clean:    { src: \"Interactive_figs/web_bar_original.png\",    cap: \"Original network performance (Clean)\" },\n    gaussian: { src: \"Interactive_figs/web_bar_gaussian.png\", cap: \"Network performance under Gaussian noise augmentations\" },\n    fgsm:     { src: \"Interactive_figs/web_bar_FGSM.png\",     cap: \"Network performance under FGSM augmentations\" },\n    sigma:    { src: \"Interactive_figs/web_bar_SIGMA.png\",    cap: \"Network performance under SIGMA augmentations\" }\n  };\n\n  const chartImg = document.getElementById(\"chartImg\");\n  const chartCap = document.getElementById(\"chartCap\");\n\n  function setActive(btn) {\n    document.querySelectorAll(\".perturb-btn\").forEach(b => {\n      const active = (b === btn);\n      b.classList.toggle(\"is-active\", active);\n      b.setAttribute(\"aria-selected\", active ? \"true\" : \"false\");\n    });\n  }\n\n  function swapChart(mode) {\n    const d = CHARTS[mode];\n\n    // nice: fade transition\n    chartImg.classList.add(\"is-fading\");\n    const next = new Image();\n    next.onload = () => {\n      chartImg.src = d.src;\n      chartImg.alt = `Bar chart for ${mode}`;\n      chartCap.textContent = d.cap;\n\n      requestAnimationFrame(() => {\n        chartImg.classList.remove(\"is-fading\");\n      });\n    };\n    next.src = d.src; // preload then swap\n  }\n\n  document.querySelectorAll(\".perturb-btn\").forEach(btn => {\n    btn.addEventListener(\"click\", () => {\n      const mode = btn.dataset.mode;\n      setActive(btn);\n      swapChart(mode);\n    });\n  });\n})();\n</script>\n```\n\n## Conclusion\n\nSIGMA offers a shift in perspective for attribution based interpretability, from explaining predictions relative to arbitrary references, to explaining them by following the model’s own confidence.\n\nBy treating explanations as paths shaped by the model itself, we gain both clearer attributions and a deeper understanding of how confidence emerges and collapses. This can also be used in model augmentation to ensure a network is more robust to noise or targeted adveserial attacks. \n\n\n---\n\n**Paper:** [Self-Guided Integrated Gradient Method for Attribution (preprint)](https://www.techrxiv.org/doi/full/10.36227/techrxiv.175037106.61341481)\n\n**Code:** [GitHub repository](https://github.com/HWQuantum/SIGMA)\n\n**Contact:** [sjh9@hw.ac.uk](mailto:sjh9@hw.ac.uk)\n\n</div>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":false,"include-in-header":[{"text":"<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700;800;900&display=swap\" rel=\"stylesheet\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n"}],"include-after-body":[{"file":"../../scripts.html"}],"number-sections":false,"output-file":"project-sigma.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"none","title":"","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}